{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlE2yy3hJROExourBZXlCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Latamila/ProjetosComAzure/blob/main/projetoTradutorDIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "PARTE I DO PROJETO\n",
        "\n",
        "Fazer um modelo para Tradução no Azure AI Services."
      ],
      "metadata": {
        "id": "RlFGh71F8RIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Dwuf06wQzDeK",
        "outputId": "6e0158e9-a877-4b09-ad0c-0dac9f331645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install requests python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from docx import Document\n",
        "import os"
      ],
      "metadata": {
        "id": "hvNjeVWLz0az"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subscription_key = \"sua chave\"\n",
        "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "location = 'eastus2'\n",
        "language_destination = 'pt-br'\n",
        "\n",
        "def translator_text(text, target_language):\n",
        "    path = '/translate'\n",
        "    constructed_url = endpoint + path\n",
        "    headers = {\n",
        "        'Ocp-Apim-Subscription-Key': subscription_key,\n",
        "        'Ocp-Apim-Subscription-Region': location,\n",
        "        'Content-type': 'application/json',\n",
        "        'X-ClientTraceId': str(os.urandom(16))\n",
        "    }\n",
        "\n",
        "    body = [{\n",
        "        'text': text\n",
        "    }]\n",
        "\n",
        "    params = {\n",
        "        'api-version': '3.0',\n",
        "        'from': 'en',\n",
        "        'to': target_language\n",
        "    }\n",
        "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "    response = request.json()\n",
        "\n",
        "    # Check if the response is a list and contains expected data before accessing it\n",
        "    if isinstance(response, list) and response and \"translations\" in response[0]:\n",
        "        return response[0][\"translations\"][0][\"text\"]\n",
        "    else:\n",
        "        # Handle the case when the expected data structure is not found\n",
        "        print(f\"Unexpected response format: {response}\")  # Print response for debugging\n",
        "        return \"Translation failed\"  # Or raise an exception"
      ],
      "metadata": {
        "id": "tGuEpKR73sUA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator_text(\"I knew you're somewhere out there, somewhere far away\", language_destination)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9xKUKf8J4Vo6",
        "outputId": "c21dc0e6-8958-4e7f-df7f-22a8f6b1751a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Eu sabia que você está em algum lugar lá fora, em algum lugar distante'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_document(path):\n",
        "    document = Document(path)\n",
        "    full_text = []\n",
        "    for paragraph in document.paragraphs:\n",
        "        translated_text = translator_text(paragraph.text, language_destination)\n",
        "        full_text.append(translated_text)\n",
        "\n",
        "    translated_doc = Document()\n",
        "    for line in full_text:\n",
        "        print(line)\n",
        "        translated_doc.add_paragraph(line)\n",
        "\n",
        "    path_translated = path.replace('.docx', f\"{language_destination}.docx\")\n",
        "    translated_doc.save(path_translated)\n",
        "    return path_translated"
      ],
      "metadata": {
        "id": "ymLuKYFl4Zf7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/musica.docx\"\n",
        "translate_document(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "OQhoPfLv6dfV",
        "outputId": "0f557428-fde7-43b7-b790-f698bd0f1658"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu sei que você está em algum lugar lá fora\n",
            "Em algum lugar distante\n",
            "Eu quero você de volta\n",
            "Eu quero você de volta\n",
            "Meus vizinhos acham que eu sou louco\n",
            "Mas eles não entendem\n",
            "Você é tudo que eu tinha\n",
            "Você é tudo que eu tinha\n",
            "À noite, quando as estrelas iluminam meu quarto\n",
            "Eu sento sozinho\n",
            "Conversando com a Lua\n",
            "Try'na chegar até você\n",
            "Na esperança de que você esteja do outro lado\n",
            "Falando comigo também\n",
            "Ou eu sou um tolo que se senta sozinho\n",
            "Falando com a Lua?\n",
            "Oh, oh\n",
            "Estou me sentindo como se fosse famoso\n",
            "O assunto da cidade\n",
            "Eles dizem que eu enlouqueci\n",
            "Sim, eu enlouqueci\n",
            "Mas eles não sabem o que eu sei\n",
            "Porque quando o sol se põe\n",
            "Alguém está respondendo\n",
            "Sim, eles estão respondendo, oh\n",
            "À noite, quando as estrelas iluminam meu quarto\n",
            "Eu sento sozinho\n",
            "Conversando com a Lua\n",
            "Try'na chegar até você\n",
            "Na esperança de que você esteja do outro lado\n",
            "Falando comigo também\n",
            "Ou eu sou um tolo que se senta sozinho\n",
            "Falando com a Lua?\n",
            "Ah, ah, ah\n",
            "Você já me ouviu chamando?\n",
            "Oh-oh-oh, oh-oh-oh (ah, ah, ah)\n",
            "Porque todas as noites\n",
            "Estou falando com a Lua\n",
            "Ainda tentando chegar até você\n",
            "Na esperança de que você esteja do outro lado\n",
            "Falando comigo também\n",
            "Ou eu sou um tolo que se senta sozinho\n",
            "Falando com a Lua?\n",
            "Oh, oh\n",
            "Eu sei que você está em algum lugar lá fora\n",
            "Em algum lugar distante\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/musicapt-br.docx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "PARTE II DO PROJETO\n",
        "\n",
        "Fazer um modelo de OPENAI\n"
      ],
      "metadata": {
        "id": "ASorS-eJ8ma5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 openai langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wYrYwidB9K2h",
        "outputId": "e0930dd0-8dcd-4a11-fd7f-d1148b8c75e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.19)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.1.143)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.0.0)\n",
            "Downloading langchain_openai-0.2.9-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.2.9 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for script_or_style in soup([\"script\", \"style\"]):\n",
        "            script_or_style.decompose()\n",
        "        texto = soup.get_text(separator = ' ')\n",
        "\n",
        "        #limpar texto\n",
        "        linhas = (line.strip() for line in texto.splitlines())\n",
        "        parts = (phrase.strip() for line in linhas for phrase in line.split(\"  \"))\n",
        "        texto_limpo = '\\n'.join(part for part in parts if part)\n",
        "        return texto_limpo\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    text = soup.get_text()\n",
        "    return text\n",
        "\n",
        "extract_text_from_url(\"https://aiperspectives.springeropen.com/articles/10.1186/s42467-020-00007-2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "_UglI78W6or8",
        "outputId": "4d8776af-0166-491c-dcc7-d0b96a6f4dc5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Can an AI learn political theory? | AI Perspectives & Advances | Full Text\\nSkip to main content\\nAdvertisement\\nSearch\\nGet published\\nExplore Journals\\nBooks\\nAbout\\nMy account\\nSearch all SpringerOpen articles\\nSearch\\nAI Perspectives & Advances\\nAbout\\nArticles\\nSubmission Guidelines\\nSubmit manuscript\\nCan an AI learn political theory?\\nDownload PDF\\nDownload PDF\\nCommentary\\nOpen access\\nPublished:\\n07 October 2020\\nCan an AI learn political theory?\\nStephen J. DeCanio\\nORCID:\\norcid.org/0000-0002-8734-4695 1 , 2\\nAI Perspectives\\nvolume \\xa02 , Article\\xa0number:\\xa0 3\\n( 2020 )\\nCite this article\\n4502\\nAccesses\\n4\\nAltmetric\\nMetrics\\ndetails\\nAbstract Alan Turing’s 1950 paper, “Computing Machinery and Intelligence,” contains much more than its proposal of the “Turing Test.” Turing imagined the development of what we today call AI by a process akin to the education of a child. Thus, while Turing anticipated “machine learning,” his prescience brings to the foreground the yet unsolved problem of how humans might teach or shape AIs to behave in ways that align with moral standards. Part of the teaching process is likely to entail AIs’ absorbing lessons from human writings. Natural language processing tools are one of the ways computer systems extract knowledge from texts. An example is given of how one such technique, Latent Dirichlet Allocation, can draw out the most prominent themes from works of classical political theory.\\nIntroduction – AI/human interactions Speculation about future interactions between artificially intelligent computer systems (AIs hereafter) and humans elicits both optimism and concern. AIs are moving beyond being mere tools that increase human capacities; in some areas like classic board games – Checkers, Chess, Shogi, and Go – they already surpass humans [ 44 ]. Footnote\\n1\\nAn AI system won one debate against an expert human debater (but also lost one), with a human audience as judges [ 41 ]. Furthermore, AI systems are beginning to exhibit genuine\\nagency ---they are able to act on their own without human intervention. Self-driving vehicles are perhaps the most prominent example of this, but autonomous weapons systems are under development by all the major military powers [ 2 ,\\n13 ] and algorithms account for considerably more than half the trading volume in the U.S. stock market. Footnote\\n2 How will future AIs relate to humans? It has been known, ever since Turing’s (1950) paper “Computing Machinery and Intelligence” [ 48 ], that even purely deterministic systems can behave in intrinsically unpredictable ways. Turing’s experience with the simple digital computer available to him at the time led to this insight:\\n[S]uppose we could be sure of finding such laws [of the behavior of computers] if they existed. Then given a discrete-state machine it should certainly be possible to discover by observation sufficient about it to predict its future behaviour, and this within a reasonable time, say a thousand years. But this does not seem to be the case. I have set up on the Manchester computer a small programme using only 1000\\u2009units of storage, whereby the machine supplied with one sixteen figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values ([ 48 ], pp. 452–453).\\nThis realization that the results of some machine computations could not be predicted in advance lay fallow for over three decades. Most of the effort to harness the power of computers focused on particular applications, and these required that outputs be generated reliably and accurately, Footnote\\n3\\nso little attention was given to the possibility of\\nintrinsic\\nunpredictability. Turing’s insight subsequently has been developed further by Wolfram [ 54 , 55 , 56 ], who formulated it in terms of “computational irreducibility.” According to Wolfram:\\n[N]ormally it has been assumed that if one can only find the underlying rules for the components of a system then in a sense these tell one everything important about the system.\\nBut what we have seen over and over again…is that this is not even close to correct, and that in fact there can be vastly more to the behavior of a system than one could ever foresee just by looking at its underlying rules. And fundamentally this is a consequence of the phenomenon of computational irreducibility ([ 56 ], p. 751).\\nComputational irreducibility means that it requires as much computational effort to know the outcome of the system being examined as it does just to let the system run to completion. In other words, no compressed or reductionist model can embody all the essentials of an irreducible system. Wolfram shows that computational irreducibility arises in even the simplest systems (e.g., cellular automata); it follows that it cannot be ruled out entirely in some larger systems. Footnote\\n4 In addition to this mathematical property, ordinary computer systems exhibiting some degree of agency will be unpredictable for more mundane reasons. For example, self-driving vehicles and other open systems absorb information from the environment that arrives in ways that cannot be predicted, and hence their behavior will be undetermined to some degree (see the papers in [ 14 ] for extended discussion of “interactive computing”). Self-motivated AIs, if such ever come to be, could interact with humans in strategic ways with outcomes that may or may not be beneficial to the humans [ 8 ]. Whether or not future AIs ever achieve genuine\\nintentionality , it will matter how they act towards and respond to humans. Such interactions, as they increasingly go beyond the simple relationship of user and tool, will have to be grounded on some kind of\\nknowledge base\\npossessed by the AI. How are future computer systems to acquire the knowledge required for moral behavior? Furthermore, what might give them the “motivation” to behave benignly? Teaching AIs to be ethical Turing addressed these questions indirectly in “Computing Machinery and Intelligence.” A significant portion (approximately the final quarter) of his paper was devoted to a discussion of “Learning Machines.” Turing speculated: In the process of trying to imitate an adult human mind we are bound to think a good deal about the process which has brought it to the state that it is in. We may notice three components,\\n(a)\\nThe initial state of the mind, say at birth,\\n(b)\\nThe education to which it has been subjected,\\n(c)\\nOther experience, not to be described as education, to which it has been subjected.\\nInstead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain….The amount of work in the education we can assume, as a first approximation, to be much the same as for the human child ([ 48 ], pp. 455–456).\\nOf course, “machine learning” is now at the cutting edge of AI research. It is not easy, however, to estimate how difficult it might be to follow Turing’s suggestion across the whole range of adult capabilities. The purely economic investment in raising human children, socializing them, and teaching them moral behavior is enormous. Childrearing entails more than imparting basic literacy and numeracy---it involves teaching children to recognize and respect other persons, to adhere to interpersonal boundaries, to train their imaginations to empathize with others, etc. Footnote\\n5 Setting aside the question of cost, the process of “bringing up baby-AI” would certainly entail some combination of building in or teaching rules of behavior, allowing for decision-making in situations where alternative actions are possible, and incorporating a knowledge base that could be drawn upon by the AI. Programming specific rules of behavior, in the spirit of a deontological ethical system, might work in many of the situations in which the AI might find itself, but could not be fully adequate to ensure moral behavior. Special cases can always be devised to confound any rigid set of rules. A consequentialist approach, by which the good and evil are judged by an action’s outcome(s), is also problematic because the complexity of human behavior and social interactions makes it impossible to predict the ultimate outcomes of actions or policies. Footnote\\n6\\nAdditionally, the end result of the educational process is itself unknowable. Human children grow up to behave in ways that cannot entirely be foreseen. Children may absorb some of the traits and values of their parents, but most assuredly have minds of their own. Child-AIs can be expected to show similar independence as they mature. In any case, part of the education of an AI will almost certainly consist of the AI’s\\nreading textual information\\non the Internet. Such a vast store of material is available, however, that some human guidance is likely to be necessary (or at least efficient) in setting the AI’s reading list. There is no simple way to measure the amount of information stored in all media because of difficulties of definition, coverage, and the rapid rate of technological progress, but a relatively recent attempt put the total amount of information stored as of 2007 at 295 optimally compressed exabytes Footnote\\n7\\n[ 17 ]. The authors noted that this amount of information stored on ordinary CD-ROMs would make a stack stretching from the Earth to the Moon and a quarter of that distance beyond (ibid., p. 62). These authors also estimated that the compound annual rate of growth of stored information had been 23% over the previous two decades; a 23% rate of growth extrapolated to 2019 would increase the 2007 figure by a factor of nearly 16. This rate of growth can probably be expected to increase over time. Faced with so much information, the curriculum of a child-AI would have to be shaped and guided by human teachers, at least at first. Given a set of “recommended readings,” a variety of methods for textual analysis that allow “M-assimilation” and “M-analysis” by computer systems without further direct human intervention. Footnote\\n8\\nThe literature on “natural language processing” has been surveyed elsewhere [ 10 ,\\n15 ,\\n28 ,\\n29 ,\\n43 ]; the techniques include Latent Semantic Analysis (LSA) [ 22 ], Latent Dirichlet Allocation and other probabilistic topic models [ 3 ,\\n47 ], neural networks [ 5 ], and a variety of other statistical methods that fall under the general category of “stylometry.” A “simple” example – political theory As an illustration, I will use one of the natural language processing approaches to see whether an AI system might begin to M-understand\\npolitical theory\\nby analyzing classical texts. Political theory is a good test case because it is so difficult; it illustrates how hard it is to think and act ethically in matters involving collective action. It hardly needs to be said that well-intentioned policies can fail, or may even produce terrible results, if the designers of the policies do not understand how the socio-political system (including economics) works. Footnote\\n9\\nSheer complexity frustrates purely technocratic efforts to solve social problems [ 12 ]. Ethical behavior in families or one-on-one interactions may be hampered by imperfect information and externalities, but these obstacles to benign outcomes are rampant in modern mass societies. In particular, the ordinary strictures against interpersonal violence must often be bypassed by the State as it carries out its necessary functions of national defense, provision of public goods through taxation, enforcement of contracts, and criminal justice. The remainder of this paper is devoted to using Latent Dirichlet Allocation (LDA) to M-analyze six classical works of political theory:\\nThe Prince\\nand\\nDiscourses on the First Ten Books of Titus Livius\\nby Niccolò Machiavelli, Aristotle’s\\nPolitics , John Locke’s\\nSecond Treatise of Government ,\\nMeditations\\nof Marcus Aurelius, and\\nThe Federalist Papers\\n(written variously by John Jay, James Madison, and Alexander Hamilton). Footnote\\n10\\nLDA will be used to extract the main topic(s) from these works and to contrast the emphases of the different authors. For imagining M-learning by processing texts, it is informative to see whether or not an AI system can extract the most salient topics from these works without any direct human guidance or intervention. LDA is based on a Bayesian model that works backwards to extract (unobserved) topics from (observed) texts under the assumption that the texts were (or could have been) generated by random sampling from particular (unobserved) probability distributions. The underlying model is a bag-of-words theory that asserts that most of the content of a text is derived from the words in association with each other, independent of structure and syntax. While the bag-of-words approach is by no means uncontroversial, “estimates of the relative amount that word order and word choice contribute to overall meaning of a sentence or paragraph suggest that the latter carries the lion’s share, on the order of 80%-90%” ([ 20 ], p. 10, citing [ 21 ] “and later”). Detailed descriptions of LDA are readily available (e.g., [ 3 ,\\n47 ]), as well as step-by-step outlines of the Monte Carlo technique (Gibbs sampling) that enables LDA to be implemented in practice [ 6 ,\\n16 ,\\n38 ]. The model is based on two underlying probability distributions: the distribution of words across a “topic,” and the distribution of topics across each document. Each topic\\ni\\nhas its own distribution of words designated by\\nφ i , and each document has a distribution of topics designated by\\nθ j . It is imagined that document\\nj\\nis generated by first picking a topic\\ni\\nat random from the distribution\\nθ j\\nand then a word at random from the distribution\\nφ i . Given a corpus of\\nN\\ndocuments, LDA estimates the (unobserved) distributions\\nφ i\\nand\\nθ j\\nby Bayesian inference. Loosely speaking, the posterior distributions of\\nφ i\\nand\\nθ j\\nconditional on the evidence can be recovered, given prior distributions for\\nφ i\\nand\\nθ j\\nand the evidence provided by the words in the actual documents in the corpus. Off-the-shelf software can carry out this computationally-intensive process (e. g., [ 25 ,\\n37 ,\\n46 ]), and the present paper uses publically downloadable Mathematica code [ 27 ]. The “documents” of the analysis were individual chapters of the six works, suitably preprocessed to eliminate capitalization, punctuation, footnotes, and the word “Chapter” at the beginning of each document. Full Porter stemming [ 35 ] was not carried out, because Porter stemming might\\xa0merge some words with the same root\\xa0that have potentially different meanings. For example, “romans” refers to a people, while “rome” is a political entity. Dirichlet priors are used in LDA, because the Dirichlet is a conjugate distribution to the Multinomial, and therefore guarantees that the posterior distributions conditioned on the data will also be Dirichlet. Use of conjugate distributions in a Bayesian framework simplifies the computation of the posterior distributions. The parameters of the Dirichlet priors were set as\\nα \\u2009=\\u20090.01 and\\nβ \\u2009=\\u20090.01; these low values of\\nα\\nand\\nβ\\nmake the distributions concentrate at the “corners” and hence will tend to sharply distinguish the topics. The number of topics was set at 40. There were a total of 387 documents: 26 chapters of\\nThe Prince , 143 chapters of the\\nDiscourses , 103 chapters of\\nPolitics , 18 chapters of the\\nSecond Treatise of Government , 12 chapters of\\nMeditations , and 85 chapters of\\nThe Federalist Papers . There is no automatic way to know whether the Gibbs sampling procedure implemented by the Mathematica code has converged to stable posterior distributions. Footnote\\n11\\nIn the present analysis, 15,000 iterations of the sampler were employed, and comparisons of different runs suggested that this number of iterations was sufficient to extract topics reliably from the documents. The results suggest that M-comprehension of the concepts of political theory is not inconceivable. Figure\\xa0 1\\nshows a plot of topic probabilities for the documents from each of the six works. Footnote\\n12\\nIn this figure, the lower axis signifies the topics (from 1 to 40), and the upper left of the box indicates the document number (from 1 to 387). The vertical axis shows the estimated posterior topic probability for each document.\\nFig. 1 topic probabilities for each of the documents Full size image Figure\\xa0 1\\nillustrates in compact form the most important (i.e., highest probability) topics for each document and author. West-to-east grid lines separate the different authors and works according to their positions in the list of 387 chapters: 1–26 for\\nThe Prince , 27–169 for the\\nDiscourses , 170–272 for the\\nPolitics , 273–290 for the\\nSecond Treatise of Government , 291–302 for\\nMeditations , and 303–387 for\\nThe Federalist Papers . South-to-north grid lines trace out the most important topics according to their probabilities. These lines follow the summits of the ridges of high probability. All the documents show high probabilities for topic 21. The 15 most frequent words in topic 21 are:\\npeople, great, time, power, shall, state, man, place, make, men, reason, government, citizens, public,\\nand\\nnature . Clearly, all the works have to do with\\npolitics\\n– the LDA algorithm is able to identify the clearest common theme. However, the works can be differentiated according to the other topics that are most prominent in them. Thus, second highest-probability topic in Machiavelli is number 38, and its 15 most frequent words are:\\nrome, prince, roman, city, romans, army, came, enemy, princes, arms, way, example, men, soldiers,\\nand\\nled . Of course we know that Machiavelli drew heavily on examples from ancient Rome in his political writings, and that\\nThe Prince\\nand the\\nDiscourses\\nconcentrate on war and conflict. By way of contrast, Aristotle’s\\nPolitics\\n(documents 170–272) in addition to “politics” (topic 21) shows high probability for topic 18:\\nevident, different, persons, manner, city, live, democracy, proper, children, oligarchy, governments, person, business, determine, established . Aristotle’s political theory addressed the advantages and disadvantages of different forms of government and offered guidelines about how humans should behave in order to prosper in society. In addition to the “politics” topic, the\\nFederalist Papers\\nshow the strong presence of topic 10:\\nconstitution, authority, political, union, governments, members, federal, danger, legislative, legislature, interests, society, rights, principles, principle . This topic embodies the\\nFederalist ’s arguments in favor of adopting the U.S. Constitution. The number of documents from Locke’s\\nSecond Treatise of Government\\nand Marcus Aurelius’\\nMeditations\\nare smaller than the number of documents from the other authors, so the signal that can be extracted from those two authors is perhaps less strong than for Machiavelli, Aristotle, and the\\nFederalist . Nevertheless, Locke and Marcus Aurelius can be identified by the small “mountains” that appear where their documents are located (positions 273 to 290 for Locke, positions 201–302 for Marcus Aurelius). The\\nMeditations , in addition to topic 21, shows some weight on topic 4:\\nuniverse, universal, needs, simplicity, rational, gods, soul, substance, plants, flesh, social, cast, beings, fate, fruit . The\\nMeditations\\nis as much a work of philosophy as of politics, and this is reflected by the words in topic 4. Locke’s\\nSecond Treatise\\nshows several other relatively low-probability topics, but it shares an emphasis on topic 10 with the authors of\\nThe Federalist Papers . This is not surprising given the influence of Locke’s political thought on the founders of the American political system. Another way of showing which topics are prominent in the different works is given in Table\\xa0 1 . This table indicates the number of times each topic is either the highest or the second-highest probability topic in the different authors’ works. The same patterns that are displayed graphically in Fig.\\n1\\nappear also in Table\\n1 . Topic 38, the one emphasizing Rome and the Romans, appears more frequently as the highest-probability topic in Machiavelli’s\\nDiscourses\\nthan in\\nThe Prince , as would be expected given that examples from the history of ancient Rome are the starting point for many of the arguments of the\\nDiscourses . The sparsity of Table\\n1\\nis attributable to the choice of Dirichlet prior parameters that makes for sharp distinctions among the topics.\\nTable 1 First and Second Most Probable Topics, Six Classical Works Full size table Conclusions Turing’s prescient thought experiment about educating a child-AI highlights just how far we are from being able to know whether actual AIs, if and when they emerge, will be benign. We have no idea “what it is like to be an AI.” Footnote\\n13\\nHuman beings are embodied creatures, and this is relevant both to learning [ 31 ,\\n51 ] Footnote\\n14\\nand to moral development. Footnote\\n15\\nIf we imagine the “motivations” of an emergent AI to be formed through a learning process, it is plausible that part of that learning will be the assimilation of human-generated works about ethics, morality, and politics. While the extraction of topics from classical texts in political theory is remote from an AI’s actually being able to formulate plans and principles for action (political or otherwise), some kind of M-understanding of such works would surely be pertinent. At the very least, the long human history of the struggle to understand politics and formulate an appropriate moral foundation for collective action reflects experiences and insights that would be relevant to an AI in dealing with the same issues. It should be emphasized that the results presented here do not (and cannot) demonstrate human-level understanding of texts by an unsupervised AI. All “bag-of-words” techniques ignore the syntactical structure of the texts. The LDA “topics” are not summaries of the contents of the works. Texts with differing or even opposing political implications might display highly similar LDA topic signatures. All of the authors whose writings are analyzed here exhibit a strong orientation to “politics,” but their philosophies are quite different. Identification of topics in texts is not sufficient to define or develop political ethics. LDA is a well-defined but circumscribed technique that enables classification and systematization of certain kinds of information contained in a set of texts. Given its limits, LDA can identify major themes---the “topics” that are its output---without human intervention. The results developed in this paper show that even corpora made up of only a few authors and works can successfully be processed by LDA. These results also show that natural language processing techniques are applicable to texts that are distant in time from the modern day. Just as an unsupervised AI can distinguish between fiction and non-fiction books [ 9 ], an AI employing LDA can extract the main topics from a set of classical texts dealing with a particular field (in this case, political theory). Deployment of more computing power would enable results like these to encompass a wider range of texts and a greater number of topics in finer detail. Footnote\\n16\\nAvailability of data and materials\\nThe texts analyzed in this paper are available in machine-readable form from Project Gutenberg ( https://www.gutenberg.org/ ), and may be freely used in the United States so long as Project Gutenberg is noted as the source. This scholarly resource is gratefully acknowledged here.\\nNotes The machine learning of Alphabet/Google’s AlphaZero takes place according to programmed rules, but produces a neural network whose internal functioning is not at all transparent. Learning systems like AlphaZero are different from the more conventional brute force, search-intensive models that solved Checkers in the 2000s and defeated the human world champion in Chess during the 1990s (for Checkers, see [ 39 ,\\n40 ]; for Chess, see [ 34 ]). Estimates range from 65 to 70% [ 18 ] to as high as 80% [ 1 ] or even 90% [ 11 ]. The best efforts of software developers have not been able to guarantee this, however. The literature on the rate of bugs per line of code is sobering [ 45 ]. In this paper Soergel estimates that “even the most careful software engineering practices in industry rarely achieve an error rate better than 1 per 1000 lines.” Nor does it imply that computationally irreducible systems are entirely random, however. Such systems can exhibit statistical regularities [ 7 ], as well as predictability at coarse levels of granularity [ 19 ]. Childrearing is perhaps the largest single investment made by society. A rough calculation of the magnitude of this effort shows its approximate magnitude. The direct cost bringing up a child in the U.S. (housing, food, etc., exclusive of college education) has been estimated to be on the order of $233,610 per child in 2015 [ 50 ], not counting the indirect cost (the time and energy spent in parenting). Approximately 16% of this direct cost estimate consists of household expenditures for “child care and education” but the estimate does not include public expenditures for education [ 24 ]. Converting the resulting direct cost number to an annual average over the entire childhood period and multiplying by the proportion of children in the population yields direct expenses to be approximately 4.7% of U.S. GDP [ 4 ,\\n49 ,\\n53 ]. The OECD reports U.S.\\npublic\\nexpenditures on education (primary through tertiary) to be 4.1% of GDP [ 33 ] and\\nprivate\\nexpenditures on education to be 2.0% [ 32 ]. (All OECD data reported here are as of 2016 or latest available and are rounded to the first decimal place.) To avoid double counting, the 16% from the USDA direct cost estimate (or approximately 0.9% of GDP) should be subtracted in computing the total resources devoted to childrearing. Alternatively, the OECD reports U.S. private spending on primary and secondary education to be 0.3% of GDP. The USDA-based estimate of this component is an overestimate because it includes “child care” costs. So even excluding the very large indirect parenting costs, the United States devotes about 10% of its national output to bringing children to full adulthood. But of course each human child needs to be brought up separately, even though there are some economies of scale in schooling, while AI “training” is likely to be much more easily transferable. These difficulties in “making machines moral” are discussed at length in [ 52 ] and the essays in [ 23 ]. One exabyte equals approximately one billion gigabytes. The convention of preceding words like “analysis,” “reading,” or “comprehension” with an “M” denotes that the cognitive operations are being performed by machines, and sidesteps the question of what constitutes “thinking” that was dismissed by Turing as “too meaningless to deserve discussion” ([ 48 ], p. 442). The use of this “M-”\\xa0terminology emphasizes that what AIs do (at least currently) is quite distinct from human understanding. Machiavelli understood this 500\\u2009years ago, although his insight is rarely acknowledged, because political decision-makers’ interests generally lie in obscuring the disconnect between rhetoric and results. This has led to “Machiavellianism” becoming shorthand for amoral or immoral politics, whereas in fact Machiavelli’s understanding that good intentions are not enough to produce good outcomes should be the starting point for the development of truly moral political practice. These books were downloaded in machine-readable form from Project Gutenberg. Raftery and Lewis suggest that in cases “reasonable accuracy may often be achieved with 5000 iterations or less” ([ 36 ], p. 1), but there is no hard-and-fast rule about this number. All three contributors to\\nThe Federalist Papers\\nwere treated as a single author. Paraphrasing the title of Nagel’s famous paper [ 30 ]. The question “[w]hat is it like to be a ____” is connected to the unsolved “hard problem” of consciousness. A compilation of perspectives on this problem is provided by the papers in [ 42 ]. As the editor puts it at the close of his Introduction, “This sort of creative diversity is of course what should be expected as we wrestle with what has come to be recognized as a serious challenge for standard materialism, namely the existence of consciousness itself” (p. 6). O’Loughlin notes that “[e]ducational theorists have often appeared to be rather uncomfortable with the brute fact of corporeality. Their discussions of cognition, social phenomena, and the development of intellectual skills or moral reasoning have been frequently carried out as if bodies were something of an embarrassment” ([ 31 ], p. 16). Of course, human morality has been a central concern of religious thinkers and philosophers for thousands of years. All the calculations reported here were performed on a desktop PC running Mathematica 12 [ 26 ]. References Amaro S (2018) Sell-offs could be down to machines that control 80% of the US stock market, fund manager says. CNBC.\\nhttps://www.cnbc.com/2018/12/05/sell-offs-could-be-down-to-machines-that-control-80percent-of-us-stocks-fund-manager-says.html . Accessed 8 June 2018. Atherton KD. Are killer robots the future of war? Parsing the facts on autonomous weapons. New York: The New York times magazine; 2018. Blei DM. Probabilistic topic models. Commun ACM. 2012;55(4):77–84. Article\\nGoogle Scholar\\nChild Trends . ( 2019 )\\nNumber of Children. Bethesda: Retrieved from\\nhttps://www.childtrends.org/indicators/number-of-children .\\nAccessed 1 Jul 2020. Collobert R, Weston J. A unified architecture for natural language processing: deep neural networks with multitask learning. In: Proceedings of the 25th international conference on machine learning. Helsinki: ACM Digital Library; 2008. Darling WM. A theoretical and practical implementation tutorial on topic modeling and Gibbs sampling: School of Computer Science, University of Guelph; 2011.\\nhttp://u.cs.biu.ac.il/~89-680/darling-lda.pdf . Accessed 9 June 2019. DeCanio SJ. Limits of economic and social knowledge. Houndmills, Basingstoke, Hampshire: Palgrave; 2014. Book\\nGoogle Scholar\\nDeCanio SJ. Games between humans and AIs. AI Soc. 2018a;33:557–64\\nhttps://link.springer.com/article/10.1007/s00146-017-0732-5 . Accessed 10 Sept 2019. Article\\nGoogle Scholar\\nDeCanio SJ. AI recognition of differences among book-length texts. AI Soc. 2018b;\\nhttps://link.springer.com/article/10.1007%2Fs00146-018-0851-7 . Accessed 10 Sept 2019. Foltz PW. Quantitative approaches to semantic knowledge representations. Discourse Process. 1998;25(2–3):127–30. Article\\nGoogle Scholar\\nFoster A (2018) How much trading in the stock market is algorithmic trading and how much is non-algorithmic? Quora.\\nhttps://www.quora.com/How-much-trading-in-the-stock-market-is-algorithmic-trading-and-how-much-is-non-algorithmic . Accessed 8 June 2018. Friedman J. Power without knowledge: a critique of technocracy. New York: Oxford University Press; 2020.\\nGoogle Scholar\\nFryer-Biggs Z. Coming soon to the battlefield: robots that can kill: The Center for Public Integrity; 2019.\\nhttps://publicintegrity.org/national-security/future-of-warfare/scary-fast/ai-warfare/ . Accessed 8 Sept 2019. Goldin D, Smolka SA, Wegner P, editors. Interactive computation: the new paradigm. Berlin: Springer-Verlag; 2006. MATH\\nGoogle Scholar\\nGomaa WH, Fahmy AA. A survey of text similarity approaches. Int J Comput Appl (0975–8887). 2013;68(13):13–8.\\nGoogle Scholar\\nHeinrich G (2009) Parameter estimation for text analysis. Technical report, Fraunhofer IGD, Darmstadt, Germany.\\nhttp://www.arbylon.net/publications/text-est2.pdf . Accessed 14 June 2019.\\nGoogle Scholar\\nHilbert M, López P. The World’s technological capacity to store, communicate, and compute information. Science. 2011;332:60–5. Article\\nGoogle Scholar\\nImburgia V. How much trading in the stock market is algorithmic trading and how much is non-algorithmic? Quora; 2018.\\nhttps://www.quora.com/How-much-trading-in-the-stock-market-is-algorithmic-trading-and-how-much-is-non-algorithmic . Accessed 8 June 2018. Israeli N, Goldenfeld N. On computational complexity and the predictability of complex physical systems: Department of Physics, University of Illinois at Urbana-Champaign; 2003.\\nhttps://arxiv.org/pdf/nlin/0309047.pdf . Accessed 10 Sept 2019. Landauer TK. LSA as a Theory of Meaning. In: Landauer TK, DS MN, Dennis S, Kintsch W, editors. Handbook of latent semantic analysis. New York: Routledge; 2007. Chapter\\nGoogle Scholar\\nLandauer TK. On the computational basis of cognition: Arguments from LSA. In: Ross BH, editor. The psychology of learning and motivation. New York: Academic Press; 2002.\\nGoogle Scholar\\nLandauer TK, McNamara DS, Dennis S, Kintsch W, editors. Handbook of latent semantic analysis. New York: Routledge; 2011.\\nGoogle Scholar\\nLin P, Abney K, Bekey GA, editors. Robot ethics: the ethical and social implications of robotics. Cambridge: The MIT Press; 2012.\\nGoogle Scholar\\nLino M, Kuczynski K, Rodriguez N, Schap T. Expenditures on Children by Families, 2015. In: Miscellaneous publication no. 1528-2015. Alexandria: U.S. Department of Agriculture, Center for Nutrition Policy and Promotion; 2017. McCallum AK (2002) MALLET: a machine learning for language toolkit.\\nhttp://mallet.cs.umass.edu . Accessed 11 June 2019.\\nGoogle Scholar\\nMathematica (Version 12) (2019) Wolfram Mathematica: The world's definitive system for modern technical computing.\\nhttp://www.wolfram.com/mathematica/ .\\nGoogle Scholar\\nMathematica Stack Exchange (2019) How to perform document classification (i.e., extracting topics from text)?\\nhttps://mathematica.stackexchange.com/questions/66987/how-to-perform-document-classification-i-e-extracting-topics-from-text . Accessed 10 Sept 2019.\\nGoogle Scholar\\nMikolov TL. Chen K, Corrado G, Dean J (2013a) Efficient estimation of word representations in vector space.\\nhttps://arxiv.org/abs/1301.3781 . Accessed 7 Sept 2019.\\nGoogle Scholar\\nMikolov T. Sutskever I, Chen K, Corrado G, Dean J (2013b) Distributed representations of words and phrases and their compositionality.\\nhttps://arxiv.org/pdf/1310.4546.pdf . Accessed 7 Sept 2019.\\nGoogle Scholar\\nNagel T. What Is It Like to Be a Bat? Philosophical Rev. 1974;83(4):435–50 Reprinted In: Nagel T (1979) Mortal Questions. Cambridge University Press, Cambridge. Article\\nGoogle Scholar\\nO’Loughlin M. Embodiment in education: exploring creatural existence. Dordrecht: Springer; 2006. Book\\nGoogle Scholar\\nOECD (Organization for Economic Co-operation and Development (2020a) Private spending on education (indicator). doi:\\nhttps://doi.org/10.1787/6e70bede-en .\\nhttps://data.oecd.org/eduresource/private-spending-on-education.htm\\nAccessed 1 Jul 2020. OECD (Organization for Economic Co-operation and Development) (2020b) Public spending on education (indicator). doi:\\nhttps://doi.org/10.1787/f99b45d0-en .\\nhttps://data.oecd.org/eduresource/public-spending-on-education.htm . Accessed 1 Jul 2020. Pandolfini B. Kasparov and deep blue: the historic chess match between man and machine. New York: Fireside; 1997.\\nGoogle Scholar\\nPorter M. An algorithm for suffix stripping. Program. 1980;14(3):130–7.\\nhttps://doi.org/10.1108/eb046814\\nAccessed 10 Sept 2019. Article\\nGoogle Scholar\\nRaftery AE, Lewis S (1991) How many iterations in the Gibbs sampler? University of Washington.\\nhttp://people.ee.duke.edu/~lcarin/raftery92how.pdf . Accessed 14 June 2019. Ramage D, Rosen E. Stanford topic modeling toolbox: The Stanford natural language processing group; 2009.\\nhttps://nlp.stanford.edu/software/tmt/tmt-0.4/ . Accessed 11 June 2019. Resnik P, Hardesty E. Gibbs sampling for the uninitiated: Semantic scholar; 2010.\\nhttps://pdfs.semanticscholar.org/6fb3/1dd73faa5aec66cfe414e235de63247e8b68.pdf?_ga=2.85528542.61205280.1560120102-1924383825.1559933214\\nAccessed 9 June 2019. Schaeffer J. One jump ahead: challenging human supremacy in checkers. New York: Springer-Verlag; 1997. Book\\nGoogle Scholar\\nSchaeffer J, Burch N, Björnsson Y, Kishimoto A, Müller M, Lake R, Lu P, Sutphen S. Checkers is solved. Science. 2007;317:1518–22. Article\\nMathSciNet\\nGoogle Scholar\\nShankland S. An IBM computer debates humans, and wins, in a new, nuanced competition: c|net; 2018.\\nhttps://www.cnet.com/news/an-ibm-computer-debates-humans-and-wins-in-a-new-nuanced-competition/ . Accessed 7 Sept 2019. Shear J, editor. Explaining Consciousness – The ‘Hard Problem.’. Cambridge: The MIT Press; 1998.\\nGoogle Scholar\\nShiffrin R, Börner K. Mapping knowledge domains. Proc Natl Acad Sci. 2004;101(suppl. 1):5183–5. Article\\nGoogle Scholar\\nSilver D, Hubert T, Schrittweiser J, Antonoglou I, Lai M, Guez A, Lanctot M, Sifre L, Kumaran D, Graepel T, Lillicrap T, Simonyan K, Hassabis D. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science. 2018;362:1140–4. Article\\nMathSciNet\\nGoogle Scholar\\nSoergel DAW. Rampant software errors may undermine scientific results [version 2; referees: 2 approved]. F1000Research. 2015;3:303\\nhttps://f1000research.com/articles/3-303 . Accessed 10 Sept 2019. Article\\nGoogle Scholar\\nSteyvers M (2011) Matlab topic modeling toolbox 1.4.\\nhttp://psiexp.ss.uci.edu/research/programs_data/toolbox.htm#Authors . Accessed 11 June 2019.\\nGoogle Scholar\\nSteyvers M, Griffiths T. Probabilistic topic models. In: Landauer TK, DS MN, Dennis S, Kintsch W, editors. (2011) Handbook of Latent Semantic Analysis. New York: Routledge; 2011. p. 427–448.48.\\nGoogle Scholar\\nTuring AM. Computing machinery and intelligence. Mind. 1950;59(236):433–60. Article\\nMathSciNet\\nGoogle Scholar\\nU.S. Bureau of Economic Analysis, Gross Domestic Product [GDP] (2020). Retrieved from FRED, Federal Reserve Bank of St. Louis;\\nhttps://fred.stlouisfed.org/series/GDP . Accessed 1 Jul 2020.\\nGoogle Scholar\\nUnited States Department of Agriculture (USDA) (2017) The Cost of Raising a Child. Posted by Mark Lino.\\nHttps://www.usda.gov/media/blog/2017/01/13/cost-raising-child . Accessed 5 Sept 2017.\\nGoogle Scholar\\nVlieghe J. The body in education. In: Smyers P, editor. International handbook of philosophy of education. Switzerland: Springer International Publishing AG, Springer International Handbooks of Education; 2018. Wallach W, Allen C. Moral machines: teaching robots right from wrong. Oxford: Oxford University Press; 2009. Book\\nGoogle Scholar\\nWorld Bank, Population, Total for United States [POPTOTUSA647NWDB] (2020). Retrieved from FRED, Federal Reserve Bank of St. Louis;\\nhttps://fred.stlouisfed.org/series/POPTOTUSA647NWDB . Accessed 1 Jul 2020. Wolfram S. Universality and complexity in cellular automata. Physica D. 1984a;10:1–35. Article\\nMathSciNet\\nGoogle Scholar\\nWolfram S. Computation theory of cellular automata. Commun Math Phys. 1984b;96:15–57. Article\\nMathSciNet\\nGoogle Scholar\\nWolfram S. A new kind of science. Champaign: Wolfram Media, Inc.; 2002. MATH\\nGoogle Scholar\\nDownload references Acknowledgments Invaluable programming assistance was provided by Nick Langston. Helpful comments and suggestions were offered by him, Alan Sanstad, and two anonymous reviewers. The usual caveats apply. Funding No external funding was relied upon for this work. Author information Authors and Affiliations Economics, Emeritus, University of California, Santa Barbara, USA Stephen J. DeCanio Tulsa, Oklahoma, USA Stephen J. DeCanio Authors Stephen J. DeCanio View author publications You can also search for this author in\\nPubMed \\xa0 Google Scholar Contributions The author read and approved the final manuscript. Corresponding author Correspondence to\\nStephen J. DeCanio . Ethics declarations\\nCompeting interests\\nNone.\\nAdditional information Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Rights and permissions\\nOpen Access\\nThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit\\nhttp://creativecommons.org/licenses/by/4.0/ .\\nReprints and permissions About this article Cite this article DeCanio, S.J. Can an AI learn political theory?.\\nAI Perspect\\n2 , 3 (2020). https://doi.org/10.1186/s42467-020-00007-2 Download citation Received :\\n10 September 2019 Accepted :\\n21 September 2020 Published :\\n07 October 2020 DOI :\\nhttps://doi.org/10.1186/s42467-020-00007-2 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Sorry, a shareable link is not currently available for this article. Copy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative\\nKeywords Artificial intelligence Machine learning Latent Dirichlet allocation Alan Turing Human/AI interactions\\nDownload PDF\\nAdvertisement\\nSupport and Contact\\nJobs\\nLanguage editing for authors\\nScientific editing for authors\\nLeave feedback\\nTerms and conditions\\nPrivacy statement\\nAccessibility\\nCookies\\nFollow SpringerOpen\\nSpringerOpen Twitter page\\nSpringerOpen Facebook page\\nBy using this website, you agree to our\\nTerms and Conditions ,\\nYour US state privacy rights ,\\nPrivacy\\nstatement\\nand\\nCookies\\npolicy.\\nYour privacy choices/Manage cookies\\nwe use in the preference centre.\\n© 2024 BioMed Central Ltd unless otherwise stated. Part of\\nSpringer Nature .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models.azure import AzureChatOpenAI"
      ],
      "metadata": {
        "id": "1Jn9yGa49yDs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = AzureChatOpenAI(\n",
        "    azure_endpoint = \"seu endpoint\",\n",
        "    api_key = \"sua chave\",\n",
        "    api_version = \"2023-07-01-preview\",\n",
        "    deployment_name = \"gpt-4o-mini\",\n",
        "    max_retries=0\n",
        ")\n",
        "\n",
        "def translate_article(text, lang):\n",
        "    messages = [\n",
        "        (\"system\" , \"Você atua como tradutor de textos\"),\n",
        "        (\"user\", f\"Traduza o {text} para o idioma {lang} e responda em markdown\")\n",
        "    ]\n",
        "\n",
        "    response = client.invoke(messages)\n",
        "    print(response.content)\n",
        "    return response.content\n",
        "\n",
        "translate_article(\"let's see if the deployment was succeeded.\", \"portugues\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CxhRhgDj_SOI",
        "outputId": "2d85a695-b011-4ea4-99a3-1f4e759cbbb4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vamos ver se a implantação foi bem-sucedida.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Vamos ver se a implantação foi bem-sucedida.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenacity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cW-j1M6FC9Mv",
        "outputId": "be3ccf97-cabc-4d84-e40f-7ff0a2d47f98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tenacity import retry, wait_exponential, stop_after_attempt\n",
        "\n",
        "# ... (your existing extract_text_from_url function) ...\n",
        "\n",
        "client = AzureChatOpenAI(\n",
        "azure_endpoint = \"seu endpoint\",\n",
        "    api_key = \"sua chave\",\n",
        "    api_version = \"2023-07-01-preview\",\n",
        "    deployment_name = \"gpt-4o-mini\",\n",
        "    max_retries=0\n",
        ")\n",
        "\n",
        "@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(6))\n",
        "def translate_article(text, lang):\n",
        "    \"\"\"\n",
        "    Translates the given text to the target language using Azure OpenAI.\n",
        "\n",
        "    This function is decorated with `retry` to automatically handle rate limit errors.\n",
        "    It will retry the request with exponential backoff, waiting for an increasing\n",
        "    amount of time between attempts (up to 6 attempts).\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        (\"system\" , \"Você atua como tradutor de textos\"),\n",
        "        (\"user\", f\"Traduza o {text} para o idioma {lang} e responda em markdown\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.invoke(messages)\n",
        "        print(response.content)\n",
        "        return response.content\n",
        "    except openai.error.RateLimitError as e:\n",
        "        print(f\"Rate limit error: {e}. Retrying...\")\n",
        "        raise  # Re-raise to trigger retry logic\n",
        "\n",
        "\n",
        "url = 'https://aiperspectives.springeropen.com/articles/10.1186/s42467-020-00007-2'\n",
        "text = extract_text_from_url(url)\n",
        "# Reduce the size of text to avoid hitting rate limit\n",
        "text = text[:500]  # Adjust the slicing to select the desired portion of the text\n",
        "article = translate_article(text, \"pt-br\")\n",
        "\n",
        "print(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5F7UwRYgSHBQ",
        "outputId": "7dbe471d-d729-4985-a893-b3798fc63ab2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Um AI pode aprender teoria política?\n",
            "\n",
            "**Stephen J. DeCanio**  \n",
            "ORCID: [orcid.org/0000-0002-8734-4695](orcid.org/0000-0002-8734-4695)\n",
            "\n",
            "**Publicado em:** 07 de outubro de 2020\n",
            "\n",
            "### Resumo\n",
            "O artigo discute a capacidade das inteligências artificiais (IA) em aprender e compreender teorias políticas. A análise envolve a avaliação dos métodos de aprendizado de máquina e sua aplicação em contextos que exigem entendimento de nuances políticas e sociais.\n",
            "\n",
            "### Palavras-chave\n",
            "- Inteligência Artificial\n",
            "- Teoria Política\n",
            "- Aprendizado de Máquina\n",
            "- Análise Crítica\n",
            "\n",
            "### Considerações Finais\n",
            "O texto conclui que, embora as IAs possam processar grandes volumes de dados e identificar padrões, sua compreensão das complexidades da teoria política ainda é limitada e requer um desenvolvimento mais profundo em termos de interpretação contextual e crítica.\n",
            "\n",
            "---\n",
            "\n",
            "Se precisar de mais informações ou detalhes sobre o conteúdo, estou à disposição!\n",
            "## Um AI pode aprender teoria política?\n",
            "\n",
            "**Stephen J. DeCanio**  \n",
            "ORCID: [orcid.org/0000-0002-8734-4695](orcid.org/0000-0002-8734-4695)\n",
            "\n",
            "**Publicado em:** 07 de outubro de 2020\n",
            "\n",
            "### Resumo\n",
            "O artigo discute a capacidade das inteligências artificiais (IA) em aprender e compreender teorias políticas. A análise envolve a avaliação dos métodos de aprendizado de máquina e sua aplicação em contextos que exigem entendimento de nuances políticas e sociais.\n",
            "\n",
            "### Palavras-chave\n",
            "- Inteligência Artificial\n",
            "- Teoria Política\n",
            "- Aprendizado de Máquina\n",
            "- Análise Crítica\n",
            "\n",
            "### Considerações Finais\n",
            "O texto conclui que, embora as IAs possam processar grandes volumes de dados e identificar padrões, sua compreensão das complexidades da teoria política ainda é limitada e requer um desenvolvimento mais profundo em termos de interpretação contextual e crítica.\n",
            "\n",
            "---\n",
            "\n",
            "Se precisar de mais informações ou detalhes sobre o conteúdo, estou à disposição!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yMz6I2xScMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}